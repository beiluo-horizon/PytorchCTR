base:
    model_name: FullMLP
    batch_size: 4096
    early_stop_patience: 2
    embedding_dim: 10
    embedding_regularizer: 1.0e-05
    net_regularizer: 0
    epochs: 100
    eval_epoch: 2
    fs_hidden_units: [800]
    learning_rate: 0.001
    loss: binary_crossentropy
    batch_normal: true
    mlp1_dropout: 0.2
    hidden_activations: relu
    mlp1_hidden_units: [400, 400, 400]
    num_workers: 16
    optimizer: adam
    
